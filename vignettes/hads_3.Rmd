---
title: "hads_3 anxiety and depression scales"
author: "Juan Li and Jim Ramsay"
date: "2023-02-22"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{hads_3 anxiety and depression scales}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(TestGardener)
```

# Introduction 

In this vignette we run through an analysis of two rating scales, the anxiety and depression components of the Hospital Anxiety and Depression Scale (hads) used by the nursing profession to assess these levels of distress of a hospitalized patient. Each of the two scales has  seven statements, and the intensity or frequency of each is rated using the integers 0, 1, 2,3 and 4. However, few patients chose level 4, and we wanted to be able to see graphic displays in three dimensions. consequently, we changed all the 4's in the data matrix to 3/s.  There 810 respondents.  The data were kindly made available to us by Prof. Ruth Ann Marrie in the College of Medicine at the University of Manitoba.

The vignette can serve as a template for the analysis of any rating scale or Likert type scale. It can also serve for tests scored using pre-assigned weights to assess the quality of the answer on something other than just "right/wrong."  

There are five distinct stages to the processing of the data:
1.  Reading in the data needed to do the analysis. 
2.  Using the data to set up a named list object containing the information essential to their analysis.
3.  The actual analysis
4.  Producing desired graphs and other displays.
5.  Simulating data in order to assess the accuracy of estimated quantities.

## Reading in the data

Here the data required for the analysis are entered.  There are up to five objects to enter:  
- a title for the analysis
- the choice data
- a character string for each item or question to display in plots
- a character string for each option choice within each item
The two essential objects are the choice data and the right answer key.  The other three are given default values.

The choice are an N by n matrix called `U` in the code.  The rows of `U` correspond to examinees and the column to items or questions.  Each number in `U` is the index of the option or answer that the examinee has chosen using the number 1 to `M` where `M` is the number of answers available for choice.  The number of choices `M` can vary from one item to another. We call this data structure *index mode* data.

The choice indices may also be stored in a dataframe where all variables are of the factor type. We have avoided this level of sophistication since the only operations on `U` are summing and retrieving values and, given the potential size of `U`, it can be worthwhile to place additional restrictions on the structure such being in integer mode.

*Note!*  Missing or illegal values in the matrix are treated as if they are an actual option or answer, and must be assigned the index `M` itself.  If these are present, then the actual designed answers are numbered from 1 to `M-1`;  otherwise `M` for an item is a proper answer choice.  TestGardener treats missing or illegal choices in exactly the same way as it treats choices of actual answers, since whether or not an proper answer was chosen is itself information about the ability or status of the examinee. 

*Note again:*  The raw data provided to a test analyzer are often in what we call *score mode* where the value indicates the score or value assigned to that choice.  This is often the case where the data are binary right/wrong values that are scored either 1 or 0, and is also often the case for rating scales.  The test analyzer will have to convert these scores to indices before submitting `U` to TestGardener.  In the binary data 1/0 case, this conversion just involves adding 1 to all the scores.  The same is the case for rating scale scores that are 0 to some larger integer.  

However, the data in the hads_3 anxiety and depression scales are already in index mode.  Added to the five possible responses is a sixth choice for missing or illegal responses, of which there are many for all symptoms.

Finally, our example here is only one of many possible ways to construct a matrix `U`, since choice data can be read in from many types of files.  The easiest case is the ".csv" file that consists of comma-separated values.  Here we read in data supplied as rows of values not separated at all, and we treat these rows as single strings.  

Load Ustr_full which is 810 by 14 and saved in a .txt file as a set of strings 14 characters long these strings contain a respondants choices for the hospital anxiety and hospital depression scale in alternating sequential order.

The reader may find it helpful to consult man pages for the functions used using the command `help("filename")` to obtain more detail and see example code.

##  Load the data required for the analysis

Load the Ustr_full, which is of length 810 and saved in file hads.txt.  If this file is not in your working folder, you will have to supply the full location of the file.

Each row in the file as a set of strings 14 characters long.  These strings are scores because they contain the score or weight of the corresponding respondant's choices for hospital anxiety and hospital depression scale with an anxiety choice followed by a depressionn choice in alternating sequential order.

```{r}
#  scan in the data as 810 strings
Ustr_full <- scan("hads.txt", "o") 
#  split the strings in separate list objects using the stringr package
Ulist <- stringr::str_split(Ustr_full,"")
#  convert the list objects to integers and store
#  as an 810 by 14 matrix
Umat <- matrix(0,810,14)
for (i in 1:810) {
  #  list convert a list to character format
  Uchari <- unlist(Ulist[[i]])
  # convert characters to integers
  Uinti  <- as.integer(Uchari)
  #  store the 14 integers in object Umat
  Umat[i,] <- Uinti
  ni     <- length(Uchari)
}
```

Choice index matrices U_anx and U_dep are extracted, and 1 is added to turn weight values 0, 1 and 2 into indicies 1, 2 and 3.  Extract the number of patients, 810, and the number of items in each scale, 7.

```{r}
U_anx <- Umat[,seq(1,13,2)] + 1
U_dep <- Umat[,seq(2,14,2)] + 1
N <- nrow(U_anx)  # 810
n <- ncol(U_anx)  #   7
```

Set up titles for the two sets data:

```{r}
titlestr_anx <- 'Anxiety (3 options)'
titlestr_dep <- 'Depression (3 options)'
```

If these scales were multiple choice tests, right answer key would be supplied here.  But
we omit this step for these scales.

##  Setting up the analysis

Now we turn to computing objects using these data that are essential to the analysis of the data.  Here we indicate that each item for both scales has exactly 3 choices, and we supply
the weights or scores 0, 1 and 2 used for each option

```{r}
noption <- 3*rep(1,n)
ScoreList <- vector("list", n)
for (item in 1:n) {
    scorei <- 0:2
    ScoreList[[item]] <- scorei
}
```

We could supply labels for each item and for each option with an item, we don't bother.
However, the object ScoreList is essential.

```{r}
optList <- list(itemLab=NULL, optLab=NULL, optScr=ScoreList)
```

Specify the number of bins used to bin the data and the sum score range

```{r}
scrrng <- c(0,21)
nbin   <- 16
```

We're now finished with the data input and setup phase.  From these objects we have a function `make_dataList` that completes the set up:

```{r}
hads_anx_dataList <- TestGardener::make.dataList(U_anx, NULL, optList)
hads_dep_dataList <- TestGardener::make.dataList(U_dep, NULL, optList)
```

The result is a named list with many members whose values may be required in the analysis phase.  Consult the documentation using `help("make.dataList.R")`.

One default option requires a bit of explanation.  Function `make.dataList()` computes sum scores for each examinee, which in the multiple choice case are simply counts of the number of correct answer choices.  The analysis phase does not use these sum scores directly.  Instead, it uses the rank orders of the sum scores (ordered from lowest to highest) divided by `N` and multipled by 100, so that the scores now look like percentages, and we refer to them as "percent ranks."

A problem is that many examinees will get the same rank value if we order these ranks directly. We cannot know whether there might be some source of bias in how these tied ranks are arranged. For example, suppose male examinees for each rank value are followed by female examinees with that rank value.  We deal with this problem by default by adding a small random normally distributed number to each tied rank that is too small to upset the original rank order.  This within-rank randomization is called "jittering" in the statistics community, and its tends to break up the influence of any source of bias.  This is the default, but can be turned off if desired.

These jittered percent ranks are used to provide initial values for a cycled optimization procedure used by TestGardener.

Let's make our first plot a display of the histograms and the probability density functions for the sum scores.

```{r,fig.width = 7}
hist(hads_anx_dataList$scrvec, hads_anx_dataList$scrrng[2], xlab="Sum Score", 
     main=titlestr_anx)
```

```{r,fig.width = 7}
hist(hads_dep_dataList$scrvec, hads_dep_dataList$scrrng[2], xlab="Sum Score",
     main=titlestr_dep)
```

We see that, on the whole, most patients do not experience great distress.

From here on, the commands and text are essentially the same as those for the SweSAT SMS data, except for comments what we see in the figures.

```{r}
theta_anx    <- hads_anx_dataList$percntrnk
thetaQnt_anx <- seq(0,100,len=2*hads_anx_dataList$nbin+1)
theta_dep    <- hads_dep_dataList$percntrnk
thetaQnt_dep <- seq(0,100,len=2*hads_dep_dataList$nbin+1)
```

## Cycling through the estimation steps

Our approach to computing optimal estimates of surprisal curves and examinee percentile rank values involves alternating between:

1. estimating surprisal curves assuming the previously computed percentile ranks are known
2. estimating examinee percentile ranks assuming the surprisal curves are known.

This is a common strategy in statistics, and especially when results are not required to be completely optimal.  We remind ourselves that nobody would need a test score that is accurate to many decimal places.  One decimal place would surely do just fine from a user's perspective.

We first choose a number of cycles that experience indicates is sufficient to achieve nearly optimal results, and then at the end of the cycles we display a measure of the total fit to the data for each cycle as a check that sufficient cycles have been carried.  Finally, we choose a cycle that appears to be sufficiently effective.  A list object is also defined that contains the various results at each cycle.

In this case the measure of total fit is the average of the fitting criterion across all examinees.  The fitting criterion for each examinee is the negative of the log of the likelihood, or *maximum likelihood estimation.*  Here we choose 10 cycles.

```{r}
ncycle <- 10
```

Here is a brief description of the steps within each cycle

###  Step 1:  Bin the data, and smooth the binned data to estimate surprisal curves

Before we invoke function `Wbinsmth`, we use the first three lines to define bin boundaries and centres so as to have a roughly equal number of examinees in each bin.  Vector `denscdfi` has already been set up that contains values of the cumulative probability distribution for the percentile ranks at a fine mesh of discrete points.  Bin locations and the locations of the five marker percents in `Qvec` are set up using interpolation.  Surprisal curves are then estimated and the results set up.
  
###  Step 2:  Compute optimal score index values

Function `thetafun()` estimates examinee percentile values given the curve shapes that we've estimated in Step 1.  The average criterion value is also computed and stored.
  

###  Step 3:  Estimate the percentile rank density 

The density that we need is only for those percentile ranks not located at either boundary, function `theta.distn()` only works with these.  The results will be used in the next cycle.

###  Step 4:  Estimate arc length scores along the test effort curve

The test information curve is the curve in the space of dimension `Wdim` that is defined by the evolution of all the curves jointly as their percentile index values range from 0 to 100%.

###  Step 5:  Set up the list object that stores results

All the results generated in this cycle are bundled together in a named list object for access at the end of the cycles.  The line saves the object in the list vector `SDS_dataResult`.
  
Here is the single command that launches the analysis:

```{r}  
AnalyzeResult_anx <- TestGardener::Analyze(theta_anx, thetaQnt_anx, hads_anx_dataList, 
                                           ncycle, itdisp=FALSE) 
AnalyzeResult_dep <- TestGardener::Analyze(theta_dep, thetaQnt_dep, hads_dep_dataList, 
                                           ncycle, itdisp=FALSE) 
```

Extract the lists containing results for each cycle

```{r}
hads_anx_parList <- AnalyzeResult_anx$parList
hads_dep_parList <- AnalyzeResult_dep$parList
```

## Plot progress of the mean fitting functions and test arc lengths 

The mean fitting values in `meanH` should decrease, and then level off as we 
approach optimal estimations of important model objects, such as optimal percent  
ranks in numeric vector `theta` and optimal surprisal curves in list vector 
`WfdList`.  Plotting these values as a function of cycle number will allow us to 
a choose best cycle for displaying results.  This will generate two plots for each set of data, so be sure that you have enough space in your Plots window in the lower right panel of RStudio.

Plots for anxiety:
```{r}
HCycle <- matrix(0,ncycle,2)
for (icycle in 1:ncycle) {
    HCycle[icycle,1] <- hads_anx_parList[[icycle]]$meanH
    HCycle[icycle,2] <- hads_anx_parList[[icycle]]$arclength
}
```

```{r,fig.width = 7}
plot(1:ncycle, HCycle[,1], type="b", lwd=2, main=titlestr_anx,
     xlab="Cycle Number", ylab="Mean H")
```

```{r,fig.width = 7}
plot(1:ncycle, HCycle[,2], type="b", lwd=2, main=titlestr_anx,
     xlab="Cycle Number", ylab="Arc Length")
```

Plots for depression:

```{r}  
HCycle <- matrix(0,ncycle,2)
for (icycle in 1:ncycle) {
    HCycle[icycle,1] <- hads_dep_parList[[icycle]]$meanH
    HCycle[icycle,2] <- hads_dep_parList[[icycle]]$arclength
}
```

```{r,fig.width = 7}
plot(1:ncycle, HCycle[,1], type="b", lwd=2, main=titlestr_dep,
     xlab="Cycle Number", ylab="Mean H")
```

```{r,fig.width = 7}
plot(1:ncycle, HCycle[,2], type="b", lwd=2, main=titlestr_dep,
     xlab="Cycle Number", ylab="Arc Length")
```

This plot shows a nice exponential-like decline in the average fitting criterion `mean H` and the arc length of the test information functtion over ten iterations.   

##  Now we choose to display results for the last cycle:

```{r}  
icycle <- 10
hads_anx_parList  <- hads_anx_parList[[icycle]]
hads_dep_parList  <- hads_dep_parList[[icycle]]
```

The list objects `hads_anx_parList` and `hads_anx_parList` contain a large number of objects, but in our exploration of results, we will only need these results:

```{r}  
WfdList_anx   <- hads_anx_parList$WfdList
theta_anx     <- hads_anx_parList$theta
Qvec_anx      <- hads_anx_parList$Qvec
arclength_anx <- hads_anx_parList$arclength
binctr_anx    <- hads_anx_parList$binctr

WfdList_dep   <- hads_dep_parList$WfdList
theta_dep     <- hads_dep_parList$theta
Qvec_dep      <- hads_dep_parList$Qvec
arclength_dep <- hads_dep_parList$arclength
binctr_dep    <- hads_dep_parList$binctr
```

###  Compute objects needed for plotting as a function of arc length or information

First we compute the quantities that we will need and save them in a named list:

```{r}
hads_anx_infoList <- TestGardener::theta2arclen(theta_anx, Qvec_anx, WfdList_anx, binctr_anx)
hads_dep_infoList <- TestGardener::theta2arclen(theta_dep, Qvec_dep, WfdList_dep, binctr_dep)
```

Then we retrieve these quantities:

```{r}
arclength_al_anx     <- hads_anx_infoList$arclength
arclengthvec_al_anx  <- hads_anx_infoList$arclengthvec
arclengthfd_al_anx   <- hads_anx_infoList$arclengthfd
theta_al_anx         <- hads_anx_infoList$theta_al
thetafine_al_anx     <- hads_anx_infoList$thetafine.rng
Qvec_al_anx          <- hads_anx_infoList$Qvec_al
binctr_al_anx        <- hads_anx_infoList$binctr_al
Wfd_info_al_anx      <- hads_anx_infoList$Wfd_info
Wdim_al_anx          <- hads_anx_infoList$Wdim

arclength_al_dep     <- hads_dep_infoList$arclength
arclengthvec_al_dep  <- hads_dep_infoList$arclengthvec
arclengthfd_al_dep   <- hads_dep_infoList$arclengthfd
theta_al_dep         <- hads_dep_infoList$theta_al
thetafine_al_dep     <- hads_dep_infoList$thetafine.rng
Qvec_al_dep          <- hads_dep_infoList$Qvec_al
binctr_al_dep        <- hads_dep_infoList$binctr_al
Wfd_info_al_dep      <- hads_dep_infoList$Wfd_info
Wdim__al_dep        <- hads_dep_infoList$Wdim
```

##       Plot the distribution of score index and information values

We plot two density functions here for the anxiety data.  The top panel is the density of the score index scores and the bottom is the density of test information scores.  This allows us to see how different the two are.  The test information plot is to be preferred because it is a ratio scale variable and allows two differences to be directly compared.

```{r,fig.width = 7}
TestGardener::density_plot(theta_anx, c(0,100), Qvec_anx, 
                           xlabstr="Score Index (Percent)", 
                           scrnbasis=11, nfine=101)
```


```{r,fig.width = 7}
TestGardener::density_plot(theta_al_anx, c(0,arclength_al_anx), Qvec_al_anx, 
                           xlabstr="Information (M-bits)", 
                           scrnbasis=15, nfine=101)
```

##  Plot surprisal curves for test questions

Here we just plot the probability and surprisal curves for the first item of the anxiety and depression data.  If argument `plotindex` is omitted, curves for all questions would be plotted.  We use the test information curve as the abscissa plotting variable because of its metric properties.

```{r,fig.width = 7}  
TestGardener::ICC.plot(arclengthvec_al_anx, WfdList_anx, hads_anx_dataList, Qvec_al_anx, 
                       data_point=TRUE, binctr_al_anx, Wrng=c(0,3), plotindex=1)
```

```{r,fig.width = 7}  
TestGardener::ICC.plot(arclengthvec_al_dep, WfdList_dep, hads_dep_dataList, Qvec_al_dep, 
                       data_point=TRUE, binctr_al_dep, Wrng=c(0,3), plotindex=1)
```

Let's make a few observations on what we see in these two plots.

When probability goes up to one, surprise declines to zero, as we would expect.  The probability a rating 0 is high and the surprisal is low if the respondent is in the bottom 25%, as we would expect.  But, for some reason that is also the case if patient is near the 75% mark.  Perhaps if the distress for other factors is that high, nausea considered of minor importance.  Or, if one is that sick, nausea is relieved by a treatment. A mild distress rating of 1 appears at the 50% level.  The probability of higher ratings is rare, and it seems that few patients at the upper end of the scale worry about this symptom.  (We convert 6-bits into 2-bits by multiplying 3 5-bits by 2.585, the value of the logarithm to the base 2 of 6.)

It is the speed of an increase or decrease in the surprisal curve that is the fundamental signal that an examinee should be boosted up and dragged down, respectively, from a given position.  The sharp increase in surprise for rating 0 at the 40% level signals that an examinee in that zone should be increased.  Of course the examinee's final actual position will depend, not only on the five surprisal curves shown here, but also on those for the remaining 12 questions.

We call the rate of increase or decrease the "sensitivity" of an option.  We have a specific plot for display this directly below.

The dots in the plot are the surprisal values for examinees in each of the 20 bins used in this analysis.  The points are on the whole close their corresponding curves, suggesting that 473 examinees gives us a pretty fair idea of the shape of a surprisal or probability curve.

##  Display test information curve projected into its first three principal components

The test information curve is in principle an object of dimension `Wdim`, but in most cases almost all of its shape can be seen in either two dimensions or three.  Here we display it in three dimensions as defined by a functional principal component analysis.

```{r, setup}
library(rgl)
options(rgl.useNULL = TRUE) # Suppress the separate window.
```

```{r,fig.width = 7,webgl=TRUE}  
Result <- TestGardener::Wpca.plot(arclength_anx, WfdList_anx, hads_anx_dataList$Wdim, 3, 
                                  titlestr=titlestr_anx, dodge = 1.1, rotate=FALSE)
rglwidget()
```

```{r,fig.width = 7,webgl=TRUE}  
Result <- TestGardener::Wpca.plot(arclength_dep, WfdList_dep, hads_dep_dataList$Wdim, 
                                  3, titlestr=titlestr_dep, dodge = 1.1, rotate=FALSE)
rglwidget()
```

There is a strong change in the direction of this curve at the 75\% marker point for the anxiety data, and at the 95\%.  Given what we saw in the density plots, this seems to be the point where the patient experiences distresses that would no longer be called normal.

